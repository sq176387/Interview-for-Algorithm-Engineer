# 目录

- [1.目标检测中IOU的相关概念与计算](#user-content-1.目标检测中iou的相关概念与计算)
- [2.目标检测中NMS的相关概念与计算](#user-content-2.目标检测中nms的相关概念与计算)
- [3.One-stage目标检测与Two-stage目标检测的区别？](#user-content-3.one-stage目标检测与two-stage目标检测的区别？)
- [4.哪些方法可以提升小目标检测的效果？](#user-content-4.哪些方法可以提升小目标检测的效果？)
- [5.ResNet模型的特点以及解决的问题？](#user-content-5.resnet模型的特点以及解决的问题？)
- [6.ResNeXt模型的结构和特点？](#user-content-6.resnext模型的结构和特点？)
- [7.MobileNet系列模型的结构和特点？](#user-content-7.mobilenet系列模型的结构和特点？)
- [8.MobileNet系列模型的结构和特点？（二）](#user-content-8.mobilenet系列模型的结构和特点？（二）)
- [9.ViT（Vision Transformer）模型的结构和特点？](#user-content-9.vit（vision-transformer）模型的结构和特点？)
- [10.EfficientNet系列模型的结构和特点？](#user-content-10.efficientnet系列模型的结构和特点？)
- [11.面试常问的经典模型？](#user-content-11.面试常问的经典模型？)
- [12.Focal Loss的作用？](#user-content-12.focal-loss的作用？)
- [13.YOLO系列的面试问题](#user-content-13.yolo系列的面试问题)
- [14.有哪些经典的轻量型人脸检测模型？](#user-content-14.有哪些经典的轻量型人脸检测模型？)
- [15.LFFD人脸检测模型的结构和特点？](#user-content-15.lffd人脸检测模型的结构和特点？)
- [16.U-Net模型的结构和特点？](#user-content-16.u-net模型的结构和特点？)
- [17.RepVGG模型的结构和特点？](#user-content-17.repvgg模型的结构和特点？)
- [18.GAN的核心思想？](#user-content-18.gan的核心思想？)
- [19.面试常问的经典GAN模型？](#user-content-19.面试常问的经典gan模型？)
- [20.FPN(Feature Pyramid Network)的相关知识](#user-content-20.fpnfeature-pyramid-network的相关知识)
- [21.SPP(Spatial Pyramid Pooling)的相关知识](#user-content-21.sppspatial-pyramid-pooling的相关知识)
- [22.目标检测中AP，AP50，AP75，mAP等指标的含义](#user-content-22.目标检测中ap，ap50，ap75，map等指标的含义)
- [23.YOLOv2中的anchor如何生成？](#user-content-23.yolov2中的anchor如何生成？)

<h2 id="1.目标检测中iou的相关概念与计算">1.目标检测中IOU的相关概念与计算</h2>
  
IoU（Intersection over Union）即交并比，是目标检测任务中一个重要的模块，其是<font color=DeepSkyBlue>GT bbox与pred bbox交集的面积 / 二者并集的面积</font>。

++++++++++++++++++++++++++++++++++++++

引申：
①、GIOU定义





++++++++++++++++++++++++++++++++++++++

![](https://files.mdnice.com/user/33499/a64f3919-f48d-402b-ad0f-8b61dd5c96eb.png)

下面我们用坐标（top，left，bottom，right），即左上角坐标，右下角坐标。从而可以在给定的两个矩形中计算IOU值。

```
def compute_iou(rect1,rect2):
  # (y0,x0,y1,x1) = (top,left,bottom,right)
  S_rect1 = (rect1[2] - rect1[0]) * (rect1[3] - rect1[1])
  S_rect2 = (rect2[2] - rect2[0]) * (rect2[3] - rect1[1])

  sum_all = S_rect1 + S_rect2
  left_line = max(rect1[1],rect2[1])
  right_line = min(rect1[3],rect2[3])
  top_line = max(rect1[0],rect2[0])
  bottom_line = min(rect1[2],rect2[2])

  if left_line >= right_line or top_line >= bottom_line:
    return 0
  else:
    intersect = (right_line - left_line) * (bottom_line - top_line)
    return (intersect / (sum_area - intersect)) * 1.0
```

<h2 id="2.目标检测中nms的相关概念与计算">2.目标检测中NMS的相关概念与计算</h2>
  
在目标检测中，我们可以利用非极大值抑制（NMS）对生成的大量候选框进行后处理，去除冗余的候选框，得到最具代表性的结果，以加快目标检测的效率。

如下图所示，消除多余的候选框，找到最佳的bbox：

![](https://files.mdnice.com/user/33499/816778a5-d6d3-42ee-bcf2-1322ee82f36a.png)

<font color=DeepSkyBlue>非极大值抑制（NMS）流程：</font>

1. 首先我们需要设置两个值：一个Score的阈值，一个IOU的阈值。

2. 对于每类对象，遍历该类的所有候选框，过滤掉Score值低于Score阈值的候选框，并根据候选框的类别分类概率进行排序：$A < B < C < D < E < F$。

3. 先标记最大概率矩形框F是我们要保留下来的候选框。

4. 从最大概率矩形框F开始，分别判断A～E与F的交并比（IOU）是否大于IOU的阈值，假设B、D与F的重叠度超过IOU阈值，那么就去除B、D。

5. 从剩下的矩形框A、C、E中，选择概率最大的E，标记为要保留下来的候选框，然后判断E与A、C的重叠度，去除重叠度超过设定阈值的矩形框。

6. 就这样重复下去，直到剩下的矩形框没有了，并标记所有要保留下来的矩形框。

7. 每一类处理完毕后，返回步骤二重新处理下一类对象。

```
import numpy as np

def py_cpu_nms(dets, thresh):
  #x1、y1（左下角坐标）、x2、y2（右上角坐标）以及score的值
  x1 = dets[:, 0]
  y1 = dets[:, 1]
  x2 = dets[:, 2]
  y2 = dets[:, 3]
  scores = dets[:, 4]

  #每一个候选框的面积
  areas = (x2 - x1 + 1) * (y2 - y1 + 1)
  #按照score降序排序（保存的是索引）
  order = scores.argsort()[::-1]

  keep = []
  while order.size > 0:
    i = order[0]
    keep.append(i)
    #计算当前概率最大矩形框与其他矩形框的相交框的坐标，会用到numpy的broadcast机制，得到向量
    xx1 = np.maximum(x1[i], x1[order[1:]])
    yy1 = np.maximum(y1[i], y1[order[1:]])
    xx2 = np.minimum(x2[i], x2[order[1:]])
    yy2 = np.minimum(y2[i], y2[order[1:]])

    #计算相交框的面积，注意矩形框不相交时w或h算出来会是负数，用0代替
    w = np.maximum(0.0, xx2 - xx1 + 1)
    h = np.maximum(0.0, yy2 - yy1 + 1)
    inter = w * h
    #计算重叠度IOU：重叠面积 / （面积1 + 面积2 - 重叠面积）
    ovr = inter / (areas[i] + areas[order[1:]] - inter)

    #找到重叠度不高于阈值的矩形框索引
    inds = np.where(ovr < thresh)[0]
    # 将order序列更新，由于前面得到的矩形框索引要比矩形框在原order序列中的索引小1，所以要加1操作
    order = order[inds + 1]

  return keep
```
  
<h2 id="3.one-stage目标检测与two-stage目标检测的区别？">3.One-stage目标检测与Two-stage目标检测的区别？</h2>

<font color=DeepSkyBlue>Two-stage目标检测算法</font>：先进行区域生成（region proposal，RP）（一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。其精度较高，速度较慢。

主要逻辑：特征提取—>生成RP—>分类/定位回归。

常见的Two-stage目标检测算法有：Faster R-CNN系列和R-FCN等。

<font color=DeepSkyBlue>One-stage目标检测算法</font>：不用RP，直接在网络中提取特征来预测物体分类和位置。其速度较快，精度比起Two-stage算法稍低。

主要逻辑：特征提取—>分类/定位回归。

常见的One-stage目标检测算法有：YOLO系列、SSD和RetinaNet等。

![](https://files.mdnice.com/user/33499/83c40e2f-8eef-4a2d-950a-22fe8c88c42c.png)

![](https://files.mdnice.com/user/33499/ce5bac5c-795e-4867-bdd4-93694bcd3e90.png)


<h2 id="4.哪些方法可以提升小目标检测的效果？">4.哪些方法可以提升小目标检测的效果？</h2>

1. 提高图像分辨率。小目标在边界框中可能只包含几个像素，那么能通过提高图像的分辨率以增加小目标的特征的丰富度。
  
2. 提高模型的输入分辨率。这是一个效果较好的通用方法，但是会带来模型inference速度变慢的问题。
  
3. 平铺图像。

![](https://files.mdnice.com/user/33499/21ddfa7a-4b39-4c1f-80f2-1907cbd6241c.png)

4. 数据增强。小目标检测增强包括随机裁剪、随机旋转和镶嵌增强等。
  
5. 自动学习anchor。
  
6. 类别优化。

<h2 id="5.resnet模型的特点以及解决的问题？">5.ResNet模型的特点以及解决的问题？</h2>
  
每次回答这个问题的时候，都会包含我的私心，我喜欢从电气自动化的角度去阐述，而非计算机角度，因为这会让我想起大学时代的青葱岁月。

ResNet就是一个<font color=DeepSkyBlue>差分放大器</font>。ResNet的结构设计，思想逻辑，就是在机器学习中抽象出了一个差分放大器，其能让深层网络的梯度的相关性增强，在进行梯度反传时突出微小的变化。

模型的特点则是设计了残差结构，其对模型输出的微小变化非常敏感。

![](https://files.mdnice.com/user/33499/0553c396-636b-4f73-be46-597c69c2e888.png)

<font color=DeepSkyBlue>为什么加入残差模块会有效果呢？</font>

假设：如果不使用残差模块，输出为$F_{1} (x)= 5.1$，期望输出为$H_{1} (x)= 5$，如果想要学习H函数，使得$F_{1} (x) = H_{1} (x) = 5$，这个变化率比较低，学习起来是比较困难的。

但是如果设计为 $H_{1} (x) = F_{1} (x) + 5 = 5.1$，进行一种拆分，使得$F_{1} (x)= 0.1$，那么学习目标就变为让$F_{1} (x)= 0$，一个映射函数学习使得它输出由0.1变为0，这个是比较简单的。也就是说引入残差模块后的映射对输出变化更加敏感了。

进一步理解：如果$F_{1} (x)= 5.1$，现在继续训练模型，使得映射函数$F_{1} (x)= 5$。变化率为：$(5.1 - 5) / 5.1 = 0.02$，如果不用残差模块的话可能要把学习率从0.01设置为0.0000001。层数少还能对付，一旦层数加深的话可能就不太好使了。

这时如果使用残差模块，也就是$F_{1} (x)= 0.1$变化为$F_{1} (x)= 0$。这个变化率增加了100%。明显这样的话对参数权重的调整作用更大。

<h2 id="6.resnext模型的结构和特点？">6.ResNeXt模型的结构和特点？</h2>
  
ResNeXt模型是在ResNet模型的基础上进行优化。其主要是在ResNeXt中引入Inception思想。如下图所示，左侧是ResNet经典结构，右侧是ResNeXt结构，<font color=DeepSkyBlue>其将单路卷积转化成多支路的多路卷积，进行分组卷积</font>。

![ResNet与ResNeXt结构对比](https://files.mdnice.com/user/33499/fb62036a-0e89-43d7-9ffb-8ad9c0357f61.png)

作者进一步提出了ResNeXt的三种等价结构，其中c结构中的分组卷积思想就跃然纸上了。

![](https://files.mdnice.com/user/33499/f0c19e6c-5d36-4cb9-9122-e0da37f084d3.png)

最后我们看一下ResNeXt50和ResNet50结构上差异的对比图：
  
![](https://files.mdnice.com/user/33499/224dd665-f4d1-4147-9b81-51e3e7737cf3.png)

ResNeXt论文：《Aggregated Residual Transformations for Deep Neural Networks》
  
<h2 id="7.mobilenet系列模型的结构和特点？">7.MobileNet系列模型的结构和特点？</h2>

MobileNet是一种轻量级的网络结构，主要针对手机等嵌入式设备而设计。MobileNetv1网络结构在VGG的基础上使用depthwise Separable卷积，在保证不损失太大精度的同时，大幅降低模型参数量。

<font color=DeepSkyBlue>Depthwise separable卷积</font>是由Depthwise卷积和Pointwise卷积构成。
Depthwise卷积(DW)能有效减少参数数量并提升运算速度。但是由于每个特征图只被一个卷积核卷积，因此经过DW输出的特征图只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。Pointwise卷积(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。
  
![Depthwise separable卷积](https://files.mdnice.com/user/33499/0ffd356d-40fc-4abc-8c0c-b40fba76e93e.png)

Depthwise Separable卷积和标准卷积的计算量对比：

![Depthwise Separable卷积：标准卷积](https://files.mdnice.com/user/33499/c1715436-23c5-48ec-be39-69583d1d4be4.png)
  
相比标准卷积，Depthwise Separable卷积可以大幅减小计算量。并且随着卷积通道数的增加，效果更加明显。

并且Mobilenetv1使用<font color=DeepSkyBlue>stride=2的卷积替换池化操作</font>，直接在卷积时利用stride=2完成了下采样，从而节省了卷积后再去用池化操作去进行一次下采样的时间，可以提升运算速度。

MobileNetv1论文:《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》

<h2 id="8.mobilenet系列模型的结构和特点？（二）">8.MobileNet系列模型的结构和特点？（二）</h2>
  
<font color=DeepSkyBlue>MobileNetV2在MobileNetV1的基础上引入了Linear Bottleneck 和 Inverted Residuals</font>。

MobileNetV2使用Linear Bottleneck(线性变换)来代替原本的非线性激活函数，来捕获感兴趣的流形。实验证明，使用Linear Bottleneck可以在小网络中较好地保留有用特征信息。

Inverted Residuals与经典ResNet残差模块的通道间操作正好相反。由于MobileNetV2使用了Linear Bottleneck结构，使其提取的特征维度整体偏低，如果只是使用低维的feature map效果并不会好。如果卷积层都是使用低维的feature map来提取特征的话，那么就没有办法提取到整体的足够多的信息。如果想要提取全面的特征信息的话，我们就需要有高维的feature map来进行补充，从而达到平衡。
  
![](https://files.mdnice.com/user/33499/3f83a7c1-7a6e-4951-9b12-4b5964300cb3.png)

MobileNetV2的论文：《MobileNetV2: Inverted Residuals and Linear Bottlenecks》

<font color=DeepSkyBlue>MobileNetV3在整体上有两大创新</font>：

1.互补搜索技术组合：由资源受限的NAS执行模块级搜索；由NetAdapt执行局部搜索，对各个模块确定之后网络层的微调。

2.网络结构改进：进一步减少网络层数，并引入h-swish激活函数。

作者发现swish激活函数能够有效提高网络的精度。然而，swish的计算量太大了。作者提出**h-swish**（hard version of swish）如下所示：
  
![](https://files.mdnice.com/user/33499/b818ab35-9b90-45eb-8648-3efadb11600f.png)

这种非线性在保持精度的情况下带了了很多优势，首先ReLU6在众多软硬件框架中都可以实现，其次量化时避免了数值精度的损失，运行快。
  
![](https://files.mdnice.com/user/33499/0de3fee3-7ed2-42c1-a3dc-0e337e6a7188.png)

MobileNetV3模型结构的优化：

![](https://files.mdnice.com/user/33499/37e41c9b-be1f-4609-bf71-5fa5a037699d.png)

MobileNetV3的论文：《Searching for MobileNetV3》

<h2 id="9.vit（vision-transformer）模型的结构和特点？">9.ViT（Vision Transformer）模型的结构和特点？</h2>

<font color=DeepSkyBlue>ViT模型特点</font>：
1.ViT直接将标准的Transformer结构直接用于图像分类，其模型结构中不含CNN。
2.为了满足Transformer输入结构要求，输入端将整个图像拆分成小图像块，然后将这些小图像块的线性嵌入序列输入网络中。在最后的输出端，使用了Class Token形式进行分类预测。
3.Transformer比CNN结构少了一定的平移不变性和局部感知性，在数据量较少的情况下，效果可能不如CNN模型，但是在大规模数据集上预训练过后，再进行迁移学习，可以在特定任务上达到SOTA性能。

<font color=DeepSkyBlue>ViT的整体模型结构</font>：

![](https://files.mdnice.com/user/33499/edb7850e-f5be-47bb-94b5-dd7d179bce6e.png)

其可以具体分成如下几个部分：
1. 图像分块嵌入
  
2. 多头注意力结构
  
3. 多层感知机结构（MLP）
  
4. 使用DropPath，Class Token，Positional Encoding等操作。

<h2 id="10.efficientnet系列模型的结构和特点？">10.EfficientNet系列模型的结构和特点？</h2>
  
Efficientnet系列模型是通过grid search从深度（depth）、宽度（width）、输入图片分辨率（resolution）三个角度共同调节搜索得来的模型。其从EfficientNet-B0到EfficientNet-L2版本，模型的精度越来越高，同样，参数量和对内存的需求也会随之变大。

深度模型的规模主要是由宽度、深度、分辨率这三个维度的缩放参数决定的。这三个维度并不是相互独立的，<font color=DeepSkyBlue>对于输入的图片分辨率更高的情况，需要有更深的网络来获得更大的感受视野。同样的，对于更高分辨率的图片，需要有更多的通道来获取更精确的特征</font>。
  
![EfficientNet模型搜索逻辑](https://files.mdnice.com/user/33499/1e645109-5248-4fc4-bd5a-9f3c05799d58.png)
  
EfficientNet模型的内部是通过多个MBConv卷积模块实现的，每个MBConv卷积模块的具体结构如下图所示。<font color=DeepSkyBlue>其用实验证明Depthwise Separable卷积在大模型中依旧非常有效；Depthwise Separable卷积较于标准卷积有更好的特征提取表达能力</font>。
  
![](https://files.mdnice.com/user/33499/43011c9e-e1b9-4f19-9ab7-71e093831aaa.png)

另外论文中使用了Drop_Connect方法来代替传统的Dropout方法来防止模型过拟合。DropConnect与Dropout不同的地方是在训练神经网络模型过程中，它不是对隐层节点的输出进行随机的丢弃，而是对隐层节点的输入进行随机的丢弃。
  
![](https://files.mdnice.com/user/33499/87bf9d15-a055-4c3c-bf3b-6391ffce4362.png)

EfficientNet论文：《EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks》

说点题外话，隔着paper都能看到作者那窒息的调参过程。。。
  
![](https://files.mdnice.com/user/33499/fce03c18-15e1-4059-b176-9761091c4983.png)

<h2 id="11.面试常问的经典模型？">11.面试常问的经典模型？</h2>
  
面试中经常会问一些关于模型方面的问题，这也是不太好量化定位的问题，因为模型繁杂多样，面试官问哪个都有可能，下面的逻辑图里我抛砖引玉列出了一些不管是在学术界还是工业界都是高价值的模型，供大家参考。

最好还是多用项目，竞赛，科研等工作润色简历，并在面试过程中将模型方面的问题引向这些工作中用到的熟悉模型里。
  
![](https://files.mdnice.com/user/33499/63dce713-b136-443d-a1f9-e93fc05cf3b0.png)

<h2 id="12.focal-loss的作用？">12.Focal Loss的作用？</h2>

<font color=DeepSkyBlue>Focal Loss是解决了分类问题中类别不均衡、分类难度差异的一个损失函数，使得模型在训练过程中更加聚焦在困难样本上。</font>

Focal Loss是从二分类问题出发，同样的思想可以迁移到多分类问题上。

我们知道二分类问题的标准loss是**交叉熵**：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621152403600.png)
对于二分类问题我们也几乎适用sigmoid激活函数$\hat{y} = \sigma(x)$，所以上面的式子可以转化成：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621153119391.png)
这里有$1 - \sigma(x) = \sigma(-x)$。

Focal Loss论文中给出的式子如下：

![](https://img-blog.csdnimg.cn/20200621165614147.png#pic_center)

其中$y\in \{ 1,-1\}$是真实标签，$p\in[0,1]$是预测概率。

我们再定义$p_{t}:$

![](https://img-blog.csdnimg.cn/20200621170205510.png#pic_center)

那么，上面的交叉熵的式子可以转换成：

![](https://img-blog.csdnimg.cn/20200621170257779.png#pic_center)

有了上面的铺垫，最初Focal Loss论文中接着引入了**均衡交叉熵函数**：

![](https://img-blog.csdnimg.cn/20200621170521537.png#pic_center)

针对类别不均衡问题，在Loss里加入一个控制权重，对于属于少数类别的样本，增大$\alpha_{t}$即可。<font color=DeepSkyBlue>但这样有一个问题，它仅仅解决了正负样本之间的平衡问题，并没有区分易分/难分样本</font>。

**为什么上述公式只解决正负样本不均衡问题呢？**

因为增加了一个系数$\alpha_{t}$，跟$p_{t}$的定义类似，当$label=1$的时候$\alpha_{t}=\alpha$ ;当$label=-1$的时候，$\alpha_{t}= 1 - \alpha$，$\alpha$的范围也是$[0,1]$。因此可以通过设定$\alpha$的值（如果$1$这个类别的样本数比$-1$这个类别的样本数少很多，那么$\alpha$可以取$0.5$到$1$来增加$1$这个类的样本的权重）来控制正负样本对整体Loss的贡献。

<h3 id="focal-loss">Focal Loss</h3>

为了可以区分难/易样本，Focal Loss雏形就出现了：

![](https://img-blog.csdnimg.cn/20200621171619978.png#pic_center)

$(1 - p_{t})^{\gamma}$用于平衡难易样本的比例不均，$\gamma >0$起到了对$(1 - p_{t})$的放大作用。$\gamma >0$减少易分样本的损失，使模型更关注于困难易错分的样本。例如当$\gamma =2$时，模型对于某正样本预测置信度$p_{t}$为$0.9$，这时$(1 - 0.9)^{\gamma} = 0.01$，也就是FL值变得很小；而当模型对于某正样本预测置信度$p_{t}$为0.3时，$(1 - 0.3)^{\gamma} = 0.49$，此时它对Loss的贡献就变大了。当$\gamma = 0$时变成交叉熵损失。

为了应对正负样本不均衡的问题，在上面的式子中再加入平衡交叉熵的$\alpha_{t}$因子，用来平衡正负样本的比例不均，最终得到Focal Loss：

![](https://img-blog.csdnimg.cn/20200621163522857.png)

Focal Loss论文中给出的实验最佳取值为$a_{t}= 0.25$，$\gamma = 2$。

![](https://img-blog.csdnimg.cn/20200621164522236.png)

<h2 id="13.yolo系列的面试问题">13.YOLO系列的面试问题</h2>

Rocky之前总结了YOLOv1-v7全系列的解析文章，帮助大家应对可能出现的与YOLO相关的面试问题，大家可按需取用：

[【Make YOLO Great Again】YOLOv1-v7全系列大解析（汇总篇）](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247486017&idx=1&sn=ff028fa235f384fe74576ef346849600&chksm=cfb4d2cef8c35bd8726cdd3bc9664f2bc5edd1eaf3876e6ff60e523277461b43a57389621bc8&token=1199271157&lang=zh_CN#rd)

<h2 id="14.有哪些经典的轻量型人脸检测模型？">14.有哪些经典的轻量型人脸检测模型？</h2>

人脸检测相对于通用目标检测来说，算是一个子任务。比起通用目标检测任务动辄检测1000个类别，人脸检测任务主要聚焦于人脸的单类目标检测，<font color=DeepSkyBlue>使用通用目标检测模型太过奢侈，有点“杀鸡用牛刀”的感觉，并且大量的参数冗余，会影响部署侧的实用性</font>，故针对人脸检测任务，学术界提出了很多轻量型的人脸检测模型，Rocky在这里给大家介绍一些比较有代表性的：

1. libfacedetection
2. Ultra-Light-Fast-Generic-Face-Detector-1MB
3. A-Light-and-Fast-Face-Detector-for-Edge-Devices
4. CenterFace
5. DBFace
6. RetinaFace
7. MTCNN

<h2 id="15.lffd人脸检测模型的结构和特点？">15.LFFD人脸检测模型的结构和特点？</h2>

<font color=DeepSkyBlue>Rocky在实习/校招面试中被多次问到LFFD模型以及面试官想套取LFFD相关算法方案的情况，说明LFFD模型在工业界还是比较有价值的</font>，下面Rocky就带着大家学习一下LFFD模型的知识：

LFFD（A-Light-and-Fast-Face-Detector-for-Edge-Devices）适用于人脸、行人、车辆等单目标检测任务，具有速度快，模型小，效果好的特点。<font color=DeepSkyBlue>LFFD是Anchor-free的方法，使用感受野替代Anchors，并在主干结构上抽取8路特征图对从小到大的人脸进行检测，检测模块分为类别二分类与边界框回归</font>。

**LFFD模型结构**

![](https://img-blog.csdnimg.cn/20200628150256899.png)

我们可以看到，LFFD模型主要由四部分组成：tiny part、small part、medium part、large part。

模型中并没有采用BN层，因为BN层会减慢17%的推理速度。其主要采用尽可能快的下采样来保持100%的人脸覆盖。

**LFFD主要特点：**

1. 结构简单直接，易于在主流AI端侧设备中进行部署。

2. 检测小目标能力突出，在极高分辨率（比如8K或更大）画面，可以检测其间10个像素大小的目标；

**LFFD损失函数**

LFFD损失函数是由regression loss和classification loss的加权和。

分类损失使用了交叉熵损失。

回归损失使用了L2损失函数。

LFFD论文地址：[LFFD: A Light and Fast Face Detector for Edge Devices论文地址](https://arxiv.org/pdf/1904.10633.pdf)

<h2 id="16.u-net模型的结构和特点？">16.U-Net模型的结构和特点？</h2>

U-Net网络结构如下所示：

![U-Net网络结构](https://img-blog.csdnimg.cn/20191127095354280.png)

U-Net网络的特点：

1. 全卷积神经网络：使用$1\times1$卷积完全取代了全连接层，使得模型的输入尺寸不受限制。
2. 左半部分网络是收缩路径（contracting path）：使用卷积和max pooling层，对feature map进行下采样。
3. 右半部分网络是扩张路径（expansive path）：使用转置卷积对feature map进行上采样，并将其与收缩路径对应层产生的特征图进行concat操作。<font color=DeepSkyBlue>上采样可以补充特征信息，加上与左半部分网络收缩路径的特征图进行concat（通过crop操作使得两个特征图尺寸一致），这就相当于在高分辨率和高维特征当中做一个融合折中</font>。
4. U-Net提出了让人耳目一新的编码器-解码器整体结构，让U-Net充满了生命力与强适应性。
 
U-Net在医疗图像，缺陷检测以及交通场景中有非常丰富的应用，可以说图像分割实际场景，U-Net是当仁不让的通用Baseline。

U-Net的论文地址：[U-Net](https://arxiv.org/pdf/1505.04597.pdf)

<h2 id="17.repvgg模型的结构和特点？">17.RepVGG模型的结构和特点？</h2>

RepVGG模型的基本架构由20多层$3\times3$卷积组成，分成5个stage，每个stage的第一层是stride=2的降采样，每个卷积层用ReLU作为激活函数。

RepVGG的主要特点：

1. $3\times3$卷积在GPU上的计算密度（理论运算量除以所用时间）可达1x1和5x5卷积的四倍.
2. 直筒型单路结构的计算效率比多路结构高。
3. 直筒型单路结构比起多路结构内存占用少。
4. 单路架构灵活性更好，容易进一步进行模型压缩等操作。
5. RepVGG中只含有一种算子，方便芯片厂商设计专用芯片来提高端侧AI效率。

那么是什么让RepVGG能在上述情形下达到SOTA效果呢？

答案就是<font color=DeepSkyBlue>结构重参数化（structural re-parameterization）</font>。

![结构重参数化逻辑](https://files.mdnice.com/user/33499/8c607b0f-8027-4df4-b30a-56e71a283afc.png)

在训练阶段，训练一个多分支模型，并将多分支模型等价转换为单路模型。在部署阶段，部署单路模型即可。这样就可以同时利用多分支模型训练时的优势（性能高）和单路模型推理时的好处（速度快、省内存）。

更多结构重参数化细节知识将在后续的篇章中展开介绍，大家尽情期待！

<h2 id="18.gan的核心思想？">18.GAN的核心思想？</h2>

2014年，Ian Goodfellow第一次提出了GAN的概念。Yann LeCun曾经说过:<font color=DeepSkyBlue>“生成对抗网络及其变种已经成为最近10年以来机器学习领域最为重要的思想之一”</font>。GAN的提出让生成式模型重新站在了深度学习这个浪潮的璀璨舞台上，与判别式模型开始谈笑风生。

GAN由生成器$G$和判别器$D$组成。其中，生成器主要负责生成相应的样本数据，输入一般是由高斯分布随机采样得到的噪声$Z$。而判别器的主要职责是区分生成器生成的样本与$gt（GroundTruth）$样本，输入一般是$gt$样本与相应的生成样本，我们想要的是对$gt$样本输出的置信度越接近$1$越好，而对生成样本输出的置信度越接近$0$越好。与一般神经网络不同的是，GAN在训练时要同时训练生成器与判别器，所以其训练难度是比较大的。

![](https://files.mdnice.com/user/33499/7f75c8e3-5a4b-4b03-9c89-84b56a66b7bb.png)

在提出GAN的第一篇论文中，生成器被比喻为印假钞票的犯罪分子，判别器则被当作警察。犯罪分子努力让印出的假钞看起来逼真，警察则不断提升对于假钞的辨识能力。二者互相博弈，随着时间的进行，都会越来越强。在图像生成任务中也是如此，生成器不断生成尽可能逼真的假图像。判别器则判断图像是$gt$图像，还是生成的图像。<font color=DeepSkyBlue>二者不断博弈优化，最终生成器生成的图像使得判别器完全无法判别真假</font>。

<font color=DeepSkyBlue>GAN的对抗思想主要由其目标函数实现</font>。具体公式如下所示：

![](https://files.mdnice.com/user/33499/6cf01c40-d0a4-40f8-9946-6228afa5936a.png)

上面这个公式看似复杂，其实不然。跳出细节来看，**整个公式的核心逻辑其实就是一个min-max问题，深度学习数学应用的边界扩展到这里，GAN便开始发光了**。

接着我们再切入细节。我们可以分两部分开看这个公式，即判别器最小化角度与生成器最大化角度。在判别器角度，我们希望最大化这个目标函数，因为在公示第一部分，其表示$gt$样本$（x ～Pdata）$输入判别器后输出的置信度，当然是越接近$1$越好。而公式的第二部分表示生成器输出的生成样本$（G(z)）$再输入判别器中进行进行二分类判别，其输出的置信度当然是越接近$0$越好，所以$1 - D(G(z))$越接近$1$越好。

在生成器角度，**我们想要最小化判别器目标函数的最大值**。判别器目标函数的最大值代表的是真实数据分布与生成数据分布的JS散度，JS散度可以度量分布的相似性，两个分布越接近，JS散度越小（JS散度是在初始GAN论文中被提出，实际应用中会发现有不足的地方，后来的论文陆续提出了很多的新损失函数来进行优化）

写到这里，大家应该就明白GAN的对抗思想了，下面是初始GAN论文中判别器与生成器损失函数的具体设置以及训练的具体流程：

![](https://files.mdnice.com/user/33499/d1813afa-8ddf-4a8d-9f4c-93b12fcaa685.png)

在图中可以看出，将判别器损失函数离散化，其与交叉熵的形式一致，我们也可以说判别器的目标是最小化交叉熵损失。

<h2 id="19.面试常问的经典gan模型？">19.面试常问的经典GAN模型？</h2>

1. 原始GAN及其训练逻辑
2. DCGAN
3. CGAN
4. WGAN
5. LSGAN
6. PixPix系列
7. CysleGAN
8. SRGAN系列

<h2 id="20.fpnfeature-pyramid-network的相关知识">20.FPN(Feature Pyramid Network)的相关知识</h2>

<h3 id="fpn的创新点">FPN的创新点</h3>

1. 设计特征金字塔的结构
2. 提取多层特征（bottom-up，top-down）
3. 多层特征融合（lateral connection）

设计特征金字塔的结构，用于解决目标检测中的多尺度问题，在基本不增加原有模型计算量的情况下，大幅度提升小物体（small object）的检测性能。

原来很多目标检测算法都是只采用高层特征进行预测，高层的特征语义信息比较丰富，但是分辨率较低，目标位置比较粗略。<font color=DeepSkyBlue>假设在深层网络中，最后的高层特征图中一个像素可能对应着输出图像$20 \times 20$的像素区域，那么小于$20 \times 20$像素的小物体的特征大概率已经丢失</font>。与此同时，低层的特征语义信息比较少，但是目标位置准确,这是对小目标检测有帮助的。<font color=DeepSkyBlue>FPN将高层特征与底层特征进行融合，从而同时利用低层特征的高分辨率和高层特征的丰富语义信息，并进行了多尺度特征的独立预测</font>，对小物体的检测效果有明显的提升。

![FPN结构](https://files.mdnice.com/user/33499/5c2e12f5-67a3-47fc-9d72-6a3495e677b8.png)

传统解决这个问题的思路包括:

1. 图像金字塔（image pyramid），即多尺度训练和测试。但该方法计算量大，耗时较久。
2. 特征分层，即每层分别输出对应的scale分辨率的检测结果，如SSD算法。但实际上不同深度对应不同层次的语义特征，浅层网络分辨率高，学到更多是细节特征，深层网络分辨率低，学到更多是语义特征，单单只有不同的特征是不够的。


<h3 id="fpn的主要模块">FPN的主要模块</h3>

1. Bottom-up pathway（自底向上线路）
2. Top-down path（自顶向下线路）
3. Lareral connections（横向链路）

![](https://files.mdnice.com/user/33499/59232cf2-31bd-4808-b3fd-7048c5dd3a59.png)

**Bottom-up pathway（自底向上线路）**

自底向上线路是卷积网络的前向传播过程。在前向传播过程中，feature map的大小可以在某些层发生改变。

**Top-down path（自顶向下线路）和Lareral connections（横向链路）**

自顶向下线路是上采样的过程，而横向链路是将自顶向下线路的结果和自底向上线路的结构进行融合。

上采样的feature map与相同大小的下采样的feature map进行逐像素相加融合（element-wise addition），其中自底向上的feature先要经过$1\times 1$卷积层，目的是为了减少通道维度。

<h3 id="fpn应用">FPN应用</h3>

论文中FPN直接在Faster R-CNN上进行改进，其backbone是ResNet101，FPN主要应用在Faster R-CNN中的RPN和Fast R-CNN两个模块中。

**FPN+RPN：**

将FPN和RPN结合起来，那RPN的输入就会变成多尺度的feature map，并且在RPN的输出侧接多个RPN head层用于满足对anchors的分类和回归。

**FPN+Fast R-CNN：**

Fast R-CNN的整体结构逻辑不变，在backbone部分引入FPN思想进行改造。

<h2 id="21.sppspatial-pyramid-pooling的相关知识">21.SPP(Spatial Pyramid Pooling)的相关知识</h2>

在目标检测领域，很多检测算法最后使用了全连接层，导致输入尺寸固定。当遇到尺寸不匹配的图像输入时，就需要使用crop或者warp等操作进行图像尺寸和算法输入的匹配。这两种方式可能出现不同的问题：裁剪的区域可能没法包含物体的整体；变形操作造成目标无用的几何失真等。

<font color=DeepSkyBlue>而SPP的做法是在卷积层后增加一个SPP layer，将features map拉成固定长度的feature vector。然后将feature vector输入到全连接层中</font>。以此来解决上述的尴尬问题。

![crop/warp与SPP的区别](https://files.mdnice.com/user/33499/cab1d4d2-e0f8-4b9f-a92d-089723b4bf69.png)

**SPP的优点：**

1. SPP可以忽略输入尺寸并且产生固定长度的输出。
2. SPP使用多种尺度的滑动核，而不是只用一个尺寸的滑动窗口进行pooling。
3. SPP在不同尺寸feature map上提取特征，增大了提取特征的丰富度。

![SPP逻辑图](https://files.mdnice.com/user/33499/36f7ca51-01d0-4f5e-bdfb-64265f8badcd.png)

在YOLOv4中，对SPP进行了创新使用，Rocky已在[【Make YOLO Great Again】YOLOv1-v7全系列大解析（Neck篇）](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247485146&idx=1&sn=f925d3509585c6cbe094e6a19cea35e2&chksm=cfb4de55f8c357439efb86d9930103614bea0c8e0adf41513cbd53d020e32972ea0a902c96c2&token=1308238350&lang=zh_CN#rd)中详细讲解，大家可按需取用～

<h2 id="22.目标检测中ap，ap50，ap75，map等指标的含义">22.目标检测中AP，AP50，AP75，mAP等指标的含义</h2>

AP：PR曲线下的面积。

![PR曲线](https://files.mdnice.com/user/33499/2bebf6d5-3270-4e8e-9a66-050678c312ea.png)

AP50: 固定IoU为50%时的AP值。

AP75:固定IoU为75%时的AP值。

AP@[0.5:0.95]:把IoU的值从50%到95%每隔5%进行了一次划分，并对这10组AP值取平均。

mAP：对所有的类别进行AP的计算，然后取均值。

mAP@[.5:.95]（即mAP@[.5,.95]）：表示在不同IoU阈值（从0.5到0.95，步长0.05）（0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95）上的平均mAP。

<h2 id="23.yolov2中的anchor如何生成？">23.YOLOv2中的anchor如何生成？</h2>

YOLOv2中<font color=DeepSkyBlue>引入K-means算法进行anchor的生成</font>，可以自动找到更好的anchor宽高的值用于模型训练的初始化。

但如果使用经典K-means中的欧氏距离作为度量，意味着较大的Anchor会比较小的Anchor产生更大的误差，聚类结果可能会偏离。

由于目标检测中主要关心anchor与ground true box（gt box）的IOU，不关心两者的大小。因此，使用IOU作为度量更加合适，即提高IOU值。因此YOLOv2采用IOU值为评判标准：

$$d(gt box,anchor) = 1 - IOU(gt box,anchor)$$

具体anchor生成步骤与经典K-means大致相同，在下一个章节中会详细介绍。主要的不同是使用的度量是$d(gt box,anchor)$，并将anchor作为簇的中心。
